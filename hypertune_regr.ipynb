{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDZdobkbwYAZ"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## hypertunes several NN architecturs on fitness data by Papkou et al., 2023   \n",
    "## for nonlinear fitness regression\n",
    "#################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "import sys\n",
    "\n",
    "#for correlation coefficients\n",
    "from scipy import stats\n",
    "\n",
    "import deep_funcs_pub as aw\n",
    "#in case aw has changed since last load\n",
    "import importlib\n",
    "importlib.reload(aw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDZdobkbwYAZ"
   },
   "outputs": [],
   "source": [
    "#setting some global parameters\n",
    "max_epochs_glob=100\n",
    "#for the hyperband tuner\n",
    "max_epochs_hptuner_glob = 10\n",
    "#training parameters for the additive nn\n",
    "#number of batches to be used for training\n",
    "batch_size_glob=128\n",
    "\n",
    "#patience to be used for training\n",
    "patience_glob=5\n",
    "\n",
    "#learning rate to be explored during tuning\n",
    "lrate_arr_glob=[1e-04, 1e-03, 1e-02]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tWnVKFkRbvj",
    "outputId": "e1918efd-c0a7-49fc-f043-bb88bf01faa8"
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "### extract fitness data\n",
    "#########################################################\n",
    "\n",
    "fitdatfile=\"fitness_data_science_papkou2023.tsv\"\n",
    "pathstr=\"\"\n",
    "filepath = pathstr + fitdatfile\n",
    "infile = Path(filepath)\n",
    "df =  pd.read_csv(infile, sep='\\t')\n",
    "\n",
    "\n",
    "fitdatall=df.T.to_dict('list')\n",
    "\n",
    "fit={}\n",
    "sefit={}\n",
    "aaseq={}\n",
    "for n in fitdatall.keys():\n",
    "    ntseq=fitdatall[n][0]\n",
    "    aaseq[ntseq]=fitdatall[n][1]\n",
    "    fit[ntseq]=fitdatall[n][2]\n",
    "    sefit[ntseq]=fitdatall[n][3]\n",
    "\n",
    "\n",
    "print(\"\\nextracted fitness data\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# hypertune a stack of dense layers\n",
    "###################################################################\n",
    "\n",
    "print(\"\\n****************************\")\n",
    "print(\"Now hypertuning dense stack\")\n",
    "print(\"\\n****************************\", flush=True)\n",
    "\n",
    "#setting random seeds for each new architecture so that these chunks can be run on their own with\n",
    "#reproducible results if needed\n",
    "random.seed(637281) \n",
    "np.random.seed(123784)\n",
    "tf.random.set_seed(243924)\n",
    "\n",
    "\n",
    "\n",
    "#prepare random training, validation, and test data\n",
    "tmpdat=aw.prep_dhfr_data_onehot_ran(aaseq, fit, sefit, flattenflag=True, \n",
    "                                 fitshift=2, f_tr=0.5, f_va=0.25, f_te=0.25, hilothresh=1.5)\n",
    "\n",
    "[alldat_ntseq, alldat_aaseq,alldat_fit,alldat_sefit,\n",
    "traindat_ntseq,traindat_aaseq,traindat_fit,traindat_sefit,\n",
    "valdat_ntseq,valdat_aaseq,valdat_fit,valdat_sefit,\n",
    "testdat_ntseq,testdat_aaseq,testdat_fit,testdat_sefit,\n",
    "traindat_ntseq_lo,traindat_aaseq_lo,traindat_fit_lo,traindat_sefit_lo,\n",
    "valdat_ntseq_lo,valdat_aaseq_lo,valdat_fit_lo,valdat_sefit_lo,\n",
    "testdat_ntseq_lo,testdat_aaseq_lo,testdat_fit_lo,testdat_sefit_lo,\n",
    "traindat_ntseq_hi,traindat_aaseq_hi,traindat_fit_hi,traindat_sefit_hi,\n",
    "valdat_ntseq_hi,valdat_aaseq_hi,valdat_fit_hi,valdat_sefit_hi,\n",
    "testdat_ntseq_hi,testdat_aaseq_hi,testdat_fit_hi,testdat_sefit_hi]=tmpdat\n",
    "\n",
    "#define convenient acronyms for training data \n",
    "#also convert fitness data to binary values since we will only classify them here\n",
    "tr_s=traindat_ntseq_hi\n",
    "tr_f=traindat_fit_hi\n",
    "va_s=valdat_ntseq_hi\n",
    "va_f=valdat_fit_hi\n",
    "te_s=testdat_ntseq_hi\n",
    "te_f=testdat_fit_hi\n",
    "\n",
    "\n",
    "#define the architecture to be hypertuned\n",
    "def dense_stack_tuner_regr(hp):\n",
    "    n_stacks=hp.Choice(f\"n_stacks\", values=[1, 2, 3, 4])\n",
    "    units=hp.Choice(f\"units\", values=[8, 16, 32, 64])\n",
    "    #apply regularization to weights, but do so uniformly across all layers\n",
    "    regu = hp.Choice(f\"reg\", values = [0.0, 1e-04, 1e-03])\n",
    "    learn_rate=hp.Choice(f\"learn_rate\", values=lrate_arr_glob)\n",
    "    dropout=hp.Choice(f\"dropout\", values=[0.0, 0.1, 0.2])\n",
    "    \n",
    "    #because data is flattened and 1-hot encoded\n",
    "    inputs = keras.Input(shape=(36, ))\n",
    "    x=layers.Dense(units = units, kernel_regularizer = regularizers.l2(regu), activation=\"relu\") (inputs)\n",
    "    stackin=layers.Dropout(dropout) (x)\n",
    "    \n",
    "    for i in range(n_stacks):\n",
    "        x=layers.Dense(units = units, kernel_regularizer = regularizers.l2(regu), activation=\"relu\") (stackin)\n",
    "        x=layers.Dropout(dropout) (x)\n",
    "        x=layers.Dense(units = units, kernel_regularizer = regularizers.l2(regu), activation=\"relu\") (x)\n",
    "        x=layers.Dropout(dropout) (x)  \n",
    "        #a residual connection and normalization step\n",
    "        x=tf.add(stackin, x)\n",
    "        stackin = layers.LayerNormalization() (x)\n",
    "    \n",
    "    outputs = layers.Dense(1) (stackin)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)   \n",
    "        \n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learn_rate), \n",
    "                  loss=\"mse\", \n",
    "                  metrics=[\"mae\", \"mape\"])\n",
    "    return model\n",
    "\n",
    "#stop if validation loss does not improve for the last patience epochs\n",
    "callbacks_list=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                              patience=patience_glob)]\n",
    "model = dense_stack_tuner_regr(kt.HyperParameters())\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "hypermodel=dense_stack_tuner_regr,\n",
    "objective=\"val_loss\",\n",
    "max_epochs=max_epochs_hptuner_glob,\n",
    "factor=3,\n",
    "hyperband_iterations=3,\n",
    "seed=None,\n",
    "overwrite=True,\n",
    "project_name=\"dense_stack_tuner_regr\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tuner.search(tr_s, tr_f, \n",
    "             epochs=max_epochs_hptuner_glob,\n",
    "             batch_size=batch_size_glob,\n",
    "             callbacks=callbacks_list,\n",
    "             validation_data=(va_s, va_f),\n",
    "             verbose=0)\n",
    "tuner.results_summary()\n",
    "\n",
    "#load the top  model\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_tuned_model = models[0]\n",
    "\n",
    "best_tuned_model.summary()\n",
    "\n",
    "\n",
    "#now train the best model for another max_epochs_glob epochs to make sure there was enough training time\n",
    "print(\"\\nnow training best model for more epochs\")\n",
    "history = best_tuned_model.fit(tr_s, tr_f,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(va_s, va_f),\n",
    "                    epochs=max_epochs_glob, \n",
    "                    batch_size=batch_size_glob, \n",
    "                    verbose=0) \n",
    "print(\"trained for additional \", len(history.history['loss']), \" epochs\")\n",
    "\n",
    "\n",
    "print(\"\\nregression quality hypertuned dense stack\")\n",
    "eval_results = best_tuned_model.evaluate(te_s, te_f, return_dict=True)\n",
    "for metric in eval_results.keys():\n",
    "    print(metric, \" on test set \", eval_results[metric])\n",
    "\n",
    "predict_fit=best_tuned_model.predict(te_s)\n",
    "\n",
    "#calculate correlation coefficients between predicted and actual fitness \n",
    "#necessary to prevent an error when calculating correlation coefficients\n",
    "predict_fit=predict_fit.flatten() \n",
    "print(\"fitness act. vs. pred. for test set\", stats.spearmanr(te_f, predict_fit), \"n=\", len(te_f))      \n",
    "print(\"fitness act. vs. pred. for test set\", stats.pearsonr(te_f, predict_fit), \"n=\", len(te_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "## now build a hypertuner for the RNN model\n",
    "###########################################\n",
    "\n",
    "print(\"\\n****************************\")\n",
    "print(\"Now hypertuning RNN\")\n",
    "print(\"\\n****************************\", flush=True)\n",
    "\n",
    "#setting random seeds for each new architecture so that these chunks can be run on their own with\n",
    "#reproducible results if needed\n",
    "random.seed(637281) \n",
    "np.random.seed(123784)\n",
    "tf.random.set_seed(243924)\n",
    "\n",
    "#prepare random training, validation, and test data\n",
    "tmpdat=aw.prep_dhfr_data_onehot_ran(aaseq, fit, sefit, flattenflag=False, \n",
    "                                 fitshift=2, f_tr=0.5, f_va=0.25, f_te=0.25, hilothresh=1.5)\n",
    "\n",
    "[alldat_ntseq, alldat_aaseq,alldat_fit,alldat_sefit,\n",
    "traindat_ntseq,traindat_aaseq,traindat_fit,traindat_sefit,\n",
    "valdat_ntseq,valdat_aaseq,valdat_fit,valdat_sefit,\n",
    "testdat_ntseq,testdat_aaseq,testdat_fit,testdat_sefit,\n",
    "traindat_ntseq_lo,traindat_aaseq_lo,traindat_fit_lo,traindat_sefit_lo,\n",
    "valdat_ntseq_lo,valdat_aaseq_lo,valdat_fit_lo,valdat_sefit_lo,\n",
    "testdat_ntseq_lo,testdat_aaseq_lo,testdat_fit_lo,testdat_sefit_lo,\n",
    "traindat_ntseq_hi,traindat_aaseq_hi,traindat_fit_hi,traindat_sefit_hi,\n",
    "valdat_ntseq_hi,valdat_aaseq_hi,valdat_fit_hi,valdat_sefit_hi,\n",
    "testdat_ntseq_hi,testdat_aaseq_hi,testdat_fit_hi,testdat_sefit_hi]=tmpdat\n",
    "\n",
    "\n",
    "#define convenient acronyms for training data \n",
    "tr_s=traindat_ntseq_hi\n",
    "tr_f=traindat_fit_hi\n",
    "va_s=valdat_ntseq_hi\n",
    "va_f=valdat_fit_hi\n",
    "te_s=testdat_ntseq_hi\n",
    "te_f=testdat_fit_hi\n",
    "\n",
    "\n",
    "#define the architecture to be hypertuned\n",
    "def RNN_tuner_regr(hp): \n",
    "    \n",
    "    num_RNN_layers=hp.Int(f\"num_intermediate_RNN_layers\", 1, 3)\n",
    "    #apply regularization to weights, but do so uniformly across all layers\n",
    "    regu = hp.Choice(\"reg\", [0.0, 1e-04, 1e-03])\n",
    "    RNN_dropout = hp.Float(\"RNNdropout\", min_value=0, max_value=0.2, step=0.1)\n",
    "    learn_rate = hp.Choice(\"learn rate\", values=lrate_arr_glob)\n",
    "    RNNunits=hp.Choice(f\"RNNunits\", values=[8, 16, 32, 48])\n",
    "        \n",
    "    input = keras.Input(shape=(9, 4))\n",
    "    # a stack of 1-3 bidirectional RNN layers\n",
    "    stackin=layers.Bidirectional(layers.LSTM(units=RNNunits,\n",
    "                                            kernel_regularizer = regularizers.l2(regu),\n",
    "                                            recurrent_regularizer = regularizers.l2(regu),\n",
    "                                            recurrent_dropout=RNN_dropout, \n",
    "                                            return_sequences=True)) (input)\n",
    "    \n",
    "    for i in range(1, num_RNN_layers+1): #note that this runs until num_RNN_layers+1-1=num_RNN_layers, \n",
    "                                         #so gives correctly the number of intermediate num_RNN_layers\n",
    "        x=layers.Bidirectional(layers.LSTM(\n",
    "                units=RNNunits,\n",
    "                kernel_regularizer = regularizers.l2(regu),\n",
    "                recurrent_regularizer = regularizers.l2(regu),\n",
    "                recurrent_dropout=RNN_dropout, \n",
    "                return_sequences=True)) (stackin)\n",
    "        #a residual connection and normalization step\n",
    "        x=tf.add(stackin, x)\n",
    "        stackin = layers.BatchNormalization() (x) \n",
    "\n",
    "    \n",
    "    #the last layer must have return sequences = False\n",
    "    x=layers.Bidirectional(layers.LSTM(\n",
    "                units=RNNunits,\n",
    "                kernel_regularizer = regularizers.l2(regu),\n",
    "                recurrent_regularizer = regularizers.l2(regu),\n",
    "                recurrent_dropout=RNN_dropout, \n",
    "                return_sequences=False)) (stackin)\n",
    "    #cannot apply residual connection below because stackin on the last layer has not the same dimension as x\n",
    "    #because of the last return sequence\n",
    "    stackin = layers.BatchNormalization() (x) \n",
    "        \n",
    "  \n",
    "    output = layers.Dense(1) (x) \n",
    "    \n",
    "    \n",
    "    model=keras.Model(inputs=input, outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learn_rate), \n",
    "                  loss=\"mse\", \n",
    "                  metrics=[\"mae\", \"mape\"])\n",
    "    return model  \n",
    "    \n",
    "\n",
    "#stop if validation loss does not improve for the last patience epochs\n",
    "callbacks_list=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                              patience=patience_glob)]\n",
    "\n",
    "model = RNN_tuner_regr(kt.HyperParameters())\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "hypermodel=RNN_tuner_regr,\n",
    "objective=\"val_loss\",\n",
    "factor=3,\n",
    "hyperband_iterations=3,\n",
    "seed=None,\n",
    "overwrite=True,\n",
    "max_epochs=max_epochs_hptuner_glob,\n",
    "project_name=\"RNN_tuner_regr\"\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tuner.search(tr_s, tr_f, \n",
    "             epochs=max_epochs_hptuner_glob,\n",
    "             batch_size=batch_size_glob,\n",
    "             callbacks=callbacks_list,\n",
    "             validation_data=(va_s, va_f),\n",
    "             verbose=0)\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# load the top  model.\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_tuned_model = models[0]\n",
    "\n",
    "best_tuned_model.summary()\n",
    "\n",
    "#now train the best model for another max_epochs_glob epochs to make sure there was enough training time\n",
    "print(\"\\nnow training best model for more epochs\")\n",
    "history = best_tuned_model.fit(tr_s, tr_f,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(va_s, va_f),\n",
    "                    epochs=max_epochs_glob, \n",
    "                    batch_size=batch_size_glob, \n",
    "                    verbose=0) \n",
    "print(\"trained for additional \", len(history.history['loss']), \" epochs\")\n",
    "\n",
    "print(\"\\nregression quality hypertuned RNN\")\n",
    "eval_results = best_tuned_model.evaluate(te_s, te_f, return_dict=True)\n",
    "for metric in eval_results.keys():\n",
    "    print(metric, \" on test set \", eval_results[metric])\n",
    "\n",
    "predict_fit=best_tuned_model.predict(te_s)\n",
    "#calculate correlation coefficients between predicted and actual fitness \n",
    "#necessary to prevent an error when calculating correlation coefficients\n",
    "predict_fit=predict_fit.flatten() \n",
    "print(\"fitness act. vs. pred. for test set\", stats.spearmanr(te_f, predict_fit), \"n=\", len(te_f))      \n",
    "print(\"fitness act. vs. pred. for test set\", stats.pearsonr(te_f, predict_fit), \"n=\", len(te_f))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "## now implement a hypertuner for a transformer stack with \n",
    "## positional embedding\n",
    "##########################################################\n",
    "\n",
    "print(\"\\n****************************\")\n",
    "print(\"Now hypertuning transformer\")\n",
    "print(\"\\n****************************\", flush=True)\n",
    "\n",
    "#setting random seeds for each new architecture so that these chunks can be run on their own with\n",
    "#reproducible results if needed\n",
    "random.seed(637281) \n",
    "np.random.seed(123784)\n",
    "tf.random.set_seed(243924)\n",
    "\n",
    "#prepare random training, validation, and test data\n",
    "#integer encode data\n",
    "tmpdat=aw.prep_dhfr_data_int_ran(aaseq, fit, sefit, fitshift=2, \n",
    "                                 f_tr=0.5, f_va=0.25, f_te=0.25, hilothresh=1.5)\n",
    "\n",
    "[alldat_ntseq, alldat_aaseq,alldat_fit,alldat_sefit,\n",
    "traindat_ntseq,traindat_aaseq,traindat_fit,traindat_sefit,\n",
    "valdat_ntseq,valdat_aaseq,valdat_fit,valdat_sefit,\n",
    "testdat_ntseq,testdat_aaseq,testdat_fit,testdat_sefit,\n",
    "traindat_ntseq_lo,traindat_aaseq_lo,traindat_fit_lo,traindat_sefit_lo,\n",
    "valdat_ntseq_lo,valdat_aaseq_lo,valdat_fit_lo,valdat_sefit_lo,\n",
    "testdat_ntseq_lo,testdat_aaseq_lo,testdat_fit_lo,testdat_sefit_lo,\n",
    "traindat_ntseq_hi,traindat_aaseq_hi,traindat_fit_hi,traindat_sefit_hi,\n",
    "valdat_ntseq_hi,valdat_aaseq_hi,valdat_fit_hi,valdat_sefit_hi,\n",
    "testdat_ntseq_hi,testdat_aaseq_hi,testdat_fit_hi,testdat_sefit_hi]=tmpdat\n",
    "\n",
    "\n",
    "#define convenient acronyms for training data \n",
    "tr_s=traindat_ntseq_hi\n",
    "tr_f=traindat_fit_hi\n",
    "va_s=valdat_ntseq_hi\n",
    "va_f=valdat_fit_hi\n",
    "te_s=testdat_ntseq_hi\n",
    "te_f=testdat_fit_hi\n",
    "\n",
    "#defining the architecture to be hypertuned\n",
    "def transf_tuner_regr(hp):\n",
    "   \n",
    "    emb_dim=hp.Choice(f\"emb_dim\", values=[2, 3, 4, 8]) \n",
    "    n_heads=hp.Choice(f\"n_heads\", values=[2, 4, 6, 8])    \n",
    "    subsp_dim=hp.Choice(f\"subsp_dim\", values=[2, 3, 4, 8])\n",
    "    dense_dim=hp.Choice(f\"dense_dim\", values=[4, 8, 16])\n",
    "    n_stacks=hp.Choice(f\"n_stacks\", values=[1, 2, 4, 6])\n",
    "    learn_rate=hp.Choice(f\"learn_rate\", values=lrate_arr_glob) \n",
    "    \n",
    "    if n_stacks<1:\n",
    "        print(\"error_aw: number of stacks invalid\")\n",
    "        exit(1)\n",
    "\n",
    "    inputs = keras.Input(shape=(9,))\n",
    "    embedded=aw.PositionalEmbedding(sequence_length=9, input_dim=4, output_dim = emb_dim) (inputs)\n",
    "    att_out = layers.MultiHeadAttention(num_heads=n_heads, key_dim=subsp_dim) (embedded, embedded, embedded)\n",
    "    x=tf.add(embedded, att_out)\n",
    "    dense_input = layers.LayerNormalization() (x) \n",
    "    x = layers.Dense(units = dense_dim, activation = 'relu') (dense_input)\n",
    "    dense_output = layers.Dense(units = emb_dim, activation = 'relu') (x)\n",
    "    #a final residual connection and normalization step\n",
    "    x=tf.add(dense_input, dense_output)\n",
    "    stack_out = layers.LayerNormalization() (x)\n",
    "    \n",
    "    #iterate this loop if there is more than one stack\n",
    "    for i in range(n_stacks):\n",
    "        att_out = layers.MultiHeadAttention(num_heads=n_heads, key_dim=subsp_dim) (stack_out, stack_out, stack_out)\n",
    "        x=tf.add(stack_out, att_out)\n",
    "        dense_input = layers.LayerNormalization() (x) \n",
    "        x = layers.Dense(units = dense_dim, activation = 'relu') (dense_input)\n",
    "        dense_output = layers.Dense(units = emb_dim, activation = 'relu') (x)\n",
    "        x=tf.add(dense_input, dense_output)\n",
    "        stack_out = layers.LayerNormalization() (x)\n",
    "        \n",
    "    if hp.Boolean(\"dropout_transformer\"):\n",
    "        stack_out=layers.Dropout(rate=0.1) (stack_out)\n",
    "    \n",
    "    #the last part here is no longer part of the transformer proper\n",
    "    #flatten the layers for the final regression output\n",
    "    x=layers.Flatten() (stack_out)\n",
    "    outputs = layers.Dense(1) (x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)   \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = learn_rate), \n",
    "                  loss=\"mse\", \n",
    "                  metrics=[\"mae\", \"mape\"])\n",
    "    return model\n",
    "    \n",
    "\n",
    "#stop if validation loss does not improve for the last patience epochs\n",
    "callbacks_list=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                              patience=patience_glob)]\n",
    "\n",
    "\n",
    "model = transf_tuner_regr(kt.HyperParameters())\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "hypermodel=transf_tuner_regr,\n",
    "objective=\"val_loss\",\n",
    "max_epochs=max_epochs_hptuner_glob,\n",
    "factor=3,\n",
    "hyperband_iterations=3,\n",
    "seed=None,\n",
    "overwrite=True, \n",
    "project_name=\"transf_tuner_regr\",\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(tr_s, tr_f, \n",
    "             epochs=max_epochs_hptuner_glob,\n",
    "             batch_size=batch_size_glob,\n",
    "             callbacks=callbacks_list,\n",
    "             validation_data=(va_s, va_f),\n",
    "             verbose=0)\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# load the top  model.\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_tuned_model = models[0]\n",
    "\n",
    "best_tuned_model.summary()\n",
    "\n",
    "#now train the best model for another max_epochs_glob epochs to make sure there was enough training time\n",
    "print(\"\\nnow training best model for more epochs\")\n",
    "history = best_tuned_model.fit(tr_s, tr_f,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(va_s, va_f),\n",
    "                    epochs=max_epochs_glob, \n",
    "                    batch_size=batch_size_glob, \n",
    "                    verbose=0) \n",
    "print(\"trained for additional \", len(history.history['loss']), \" epochs\")\n",
    "\n",
    "print(\"\\nregression quality transformed tuned\")\n",
    "eval_results = best_tuned_model.evaluate(te_s, te_f, return_dict=True)\n",
    "for metric in eval_results.keys():\n",
    "    print(metric, \" on test set \", eval_results[metric])\n",
    "\n",
    "predict_fit=best_tuned_model.predict(te_s)\n",
    "#calculate correlation coefficients between predicted and actual fitness \n",
    "#necessary to prevent an error when calculating correlation coefficients\n",
    "predict_fit=predict_fit.flatten() \n",
    "print(\"fitness act. vs. pred. for test set\", stats.spearmanr(te_f, predict_fit), \"n=\", len(te_f))      \n",
    "print(\"fitness act. vs. pred. for test set\", stats.pearsonr(te_f, predict_fit), \"n=\", len(te_f))      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## now hypertune a convnet \n",
    "## based on anly the nucleotide features, flattened \n",
    "##########################################################\n",
    "\n",
    "print(\"\\n****************************\")\n",
    "print(\"Now hypertuning convnet\")\n",
    "print(\"\\n****************************\", flush=True)\n",
    "\n",
    "\n",
    "#setting random seeds for each new architecture so that these chunks can be run on their own with\n",
    "#reproducible results if needed\n",
    "random.seed(637281) \n",
    "np.random.seed(123784)\n",
    "tf.random.set_seed(243924)\n",
    "\n",
    "\n",
    "#prepare random training, validation, and test data, flattendc 1-hot-encoded\n",
    "tmpdat=aw.prep_dhfr_data_onehot_ran(aaseq, fit, sefit, flattenflag=True, \n",
    "                                 fitshift=2, f_tr=0.5, f_va=0.25, f_te=0.25, hilothresh=1.5)\n",
    "\n",
    "[alldat_ntseq, alldat_aaseq,alldat_fit,alldat_sefit,\n",
    "traindat_ntseq,traindat_aaseq,traindat_fit,traindat_sefit,\n",
    "valdat_ntseq,valdat_aaseq,valdat_fit,valdat_sefit,\n",
    "testdat_ntseq,testdat_aaseq,testdat_fit,testdat_sefit,\n",
    "traindat_ntseq_lo,traindat_aaseq_lo,traindat_fit_lo,traindat_sefit_lo,\n",
    "valdat_ntseq_lo,valdat_aaseq_lo,valdat_fit_lo,valdat_sefit_lo,\n",
    "testdat_ntseq_lo,testdat_aaseq_lo,testdat_fit_lo,testdat_sefit_lo,\n",
    "traindat_ntseq_hi,traindat_aaseq_hi,traindat_fit_hi,traindat_sefit_hi,\n",
    "valdat_ntseq_hi,valdat_aaseq_hi,valdat_fit_hi,valdat_sefit_hi,\n",
    "testdat_ntseq_hi,testdat_aaseq_hi,testdat_fit_hi,testdat_sefit_hi]=tmpdat\n",
    "\n",
    "#define convenient acronyms for training data \n",
    "tr_s=traindat_ntseq_hi\n",
    "tr_f=traindat_fit_hi\n",
    "va_s=valdat_ntseq_hi\n",
    "va_f=valdat_fit_hi\n",
    "te_s=testdat_ntseq_hi\n",
    "te_f=testdat_fit_hi\n",
    "\n",
    "\n",
    "\n",
    "#some parameters for the first layer\n",
    "#make the kernel size for the convolution span at least two nucleotides =8 position in the hot-encoding \n",
    "ker_size = 8\n",
    "print(\"kernel size \", ker_size)\n",
    "#number of filters for first convnet layer, same as possible dinucleotide combinations\n",
    "n_filters=16\n",
    "print(\"convolutional filters \", n_filters)\n",
    "#use a minimal stride of one nucleotide = 4 positions in the flattened \n",
    "convstride=4\n",
    "print(\"stride for convolution\", convstride)\n",
    "\n",
    "#define the architecture to be hypertuned\n",
    "def convnet_tuner_regr(hp):\n",
    "    n_conv_layers=hp.Choice(f\"conv layers\", values=[2, 3,4, 5,6,7])\n",
    "    n_dense_layers=hp.Choice(f\"dense layers\", values=[1,2,3])\n",
    "    n_units_dense=hp.Choice(f\"units\", values=[8, 16,32])\n",
    "    learn_rate=hp.Choice(f\"learn_rate\", values=lrate_arr_glob)\n",
    "    regul=hp.Choice(f\"regul\", values=[0.0, 1e-04, 1e-03])\n",
    "    \n",
    "    #a word is 9x4 nts long\n",
    "    inputs = keras.Input(shape=(36,))\n",
    "   \n",
    "    #the convnet needs an input where the last dimension equals the \"channel\" and is one, so need to \n",
    "    #create that channel\n",
    "    x=layers.Reshape((36, 1)) (inputs)\n",
    "    \n",
    "    x=layers.Conv1D(filters = n_filters, kernel_size = ker_size, strides=convstride, activation='relu')(x)\n",
    "    filtarr=[24,32, 48,64,96,128]\n",
    "    for i in range(1, n_conv_layers):\n",
    "        x=layers.Conv1D(filters = filtarr[i-1], kernel_size = 2, strides=1, activation='relu')(x)\n",
    "    \n",
    "    #flatten the output to feed into a dense layer\n",
    "    x=layers.Flatten() (x) \n",
    "    #now add dense layers    \n",
    "    for i in range(n_dense_layers):\n",
    "        x=layers.Dense(n_units_dense, kernel_regularizer = regularizers.l2(regul), activation=\"relu\") (x)\n",
    "    \n",
    "    \n",
    "    outputs = layers.Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)   \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learn_rate), \n",
    "                  loss=\"mse\", \n",
    "                  metrics=[\"mae\", \"mape\"])\n",
    "    return model\n",
    "\n",
    " \n",
    "#stop if validation loss does not improve for the last patience epochs\n",
    "callbacks_list=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                              patience=patience_glob)]\n",
    "model = convnet_tuner_regr(kt.HyperParameters())\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "hypermodel=convnet_tuner_regr, \n",
    "objective=\"val_loss\",\n",
    "max_epochs=max_epochs_hptuner_glob,\n",
    "factor=3,\n",
    "hyperband_iterations=1,\n",
    "seed=None,\n",
    "overwrite=True,\n",
    "project_name=\"convnet_tuner_regr\",\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(tr_s, tr_f, \n",
    "             epochs=max_epochs_hptuner_glob, \n",
    "             batch_size=batch_size_glob,\n",
    "             callbacks=callbacks_list,\n",
    "             validation_data=(va_s, va_f),\n",
    "             verbose=0)\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# load the top  model.\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_tuned_model = models[0]\n",
    "\n",
    "best_tuned_model.summary()\n",
    "\n",
    "#now train the best model for another max_epochs_glob epochs to make sure there was enough training time\n",
    "print(\"\\nnow training best model for more epochs\")\n",
    "history = best_tuned_model.fit(tr_s, tr_f,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(va_s, va_f),\n",
    "                    epochs=max_epochs_glob, \n",
    "                    batch_size=batch_size_glob, \n",
    "                    verbose=0) \n",
    "print(\"trained for additional \", len(history.history['loss']), \" epochs\")\n",
    "\n",
    "print(\"\\nregression quality hypertuned convnet\")\n",
    "eval_results = best_tuned_model.evaluate(te_s, te_f, return_dict=True)\n",
    "for metric in eval_results.keys():\n",
    "    print(metric, \" on test set \", eval_results[metric])\n",
    "\n",
    "predict_fit=best_tuned_model.predict(te_s)\n",
    "#calculate correlation coefficients between predicted and actual fitness \n",
    "#necessary to prevent an error when calculating correlation coefficients\n",
    "predict_fit=predict_fit.flatten() \n",
    "\n",
    "print(\"fitness act. vs. pred. for test set\", stats.spearmanr(te_f, predict_fit), \"n=\", len(te_f))      \n",
    "print(\"fitness act. vs. pred. for test set\", stats.pearsonr(te_f, predict_fit), \"n=\", len(te_f))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "## hypertune codon-based RNN with positional embedding\n",
    "#######################################################\n",
    "\n",
    "print(\"\\n*******************************************************\")\n",
    "print(\"Now hypertuning codon-based RNN with positional embedding\")\n",
    "print(\"\\n********************************************************\", flush=True)\n",
    "\n",
    "\n",
    "#setting random seeds for each new architecture so that these chunks can be run on their own with\n",
    "#reproducible results if needed\n",
    "random.seed(637281) \n",
    "np.random.seed(123784)\n",
    "tf.random.set_seed(243924)\n",
    "\n",
    "\n",
    "#prepare random training, validation, and test data\n",
    "#integer-encode the ntseq, the aa seq and the codon sequence\n",
    "tmpdat=aw.prep_dhfr_data_int_codon_ran(aaseq, fit, sefit, fitshift=2, f_tr=0.5, f_va=0.25, f_te=0.25, hilothresh=1.5)\n",
    "[alldat_ntseq, alldat_aaseq,alldat_codseq,alldat_fit,alldat_sefit,\n",
    "traindat_ntseq,traindat_aaseq,traindat_codseq, traindat_fit,traindat_sefit,\n",
    "valdat_ntseq,valdat_aaseq,valdat_codseq, valdat_fit,valdat_sefit,\n",
    "testdat_ntseq,testdat_aaseq,testdat_codseq, testdat_fit,testdat_sefit,\n",
    "traindat_ntseq_lo,traindat_aaseq_lo,traindat_codseq_lo,traindat_fit_lo,traindat_sefit_lo,\n",
    "valdat_ntseq_lo,valdat_aaseq_lo,valdat_codseq_lo,valdat_fit_lo,valdat_sefit_lo,\n",
    "testdat_ntseq_lo,testdat_aaseq_lo,testdat_codseq_lo,testdat_fit_lo,testdat_sefit_lo,\n",
    "traindat_ntseq_hi,traindat_aaseq_hi,traindat_codseq_hi,traindat_fit_hi,traindat_sefit_hi,\n",
    "valdat_ntseq_hi,valdat_aaseq_hi,valdat_codseq_hi,valdat_fit_hi,valdat_sefit_hi,\n",
    "testdat_ntseq_hi,testdat_aaseq_hi,testdat_codseq_hi,testdat_fit_hi,testdat_sefit_hi]=tmpdat\n",
    "\n",
    "#define convenient acronyms for training data \n",
    "tr_s=traindat_ntseq_hi\n",
    "tr_c=traindat_codseq_hi\n",
    "tr_f=traindat_fit_hi\n",
    "va_s=valdat_ntseq_hi\n",
    "va_c=valdat_codseq_hi\n",
    "va_f=valdat_fit_hi\n",
    "te_s=testdat_ntseq_hi\n",
    "te_c=testdat_codseq_hi\n",
    "te_f=testdat_fit_hi\n",
    "\n",
    "\n",
    "#define the architecture to be hypertuned\n",
    "#treat the codon data as if it was a 64-dimensional time series of length 3\n",
    "def RNN_cod_pos_tuner_regr(hp):\n",
    "    \n",
    "    emb_dim=hp.Choice(f\"emb_dim\", values=[4, 8, 16, 32])   \n",
    "    #note that the first RNN layer but not the first dense layer is treated differently\n",
    "    #to allow for residual connection\n",
    "   \n",
    "    num_RNN_layers=hp.Int(f\"num_intermediate_RNN_layers\", 1, 3)\n",
    "    RNNunits=hp.Choice(f\"RNNunits\", values=[8, 16, 32, 48])\n",
    "\n",
    "    #apply regularization to weights, but do so uniformly across all layers\n",
    "    regu = hp.Choice(\"reg\", [0.0, 1e-04, 1e-03])\n",
    "    RNN_dropout = hp.Float(\"RNNdropout\", min_value=0, max_value=0.2, step=0.1)\n",
    "    learn_rate = hp.Choice(\"learn rate\", values=lrate_arr_glob)\n",
    "        \n",
    "\n",
    "    inputs = keras.Input(shape=(3,))\n",
    "    embedded=aw.PositionalEmbedding(sequence_length=3, input_dim=64, output_dim = emb_dim) (inputs)\n",
    "    \n",
    "    \n",
    "    stackin=layers.Bidirectional(layers.LSTM(units=RNNunits,\n",
    "                                            kernel_regularizer = regularizers.l2(regu),\n",
    "                                            recurrent_regularizer = regularizers.l2(regu),\n",
    "                                            recurrent_dropout=RNN_dropout, \n",
    "                                            return_sequences=True)) (embedded)\n",
    "    \n",
    "    for i in range(1, num_RNN_layers+1): #note that this runs until num_RNN_layers+1-1=num_RNN_layers, \n",
    "                                         #so gives correctly the number of intermediate num_RNN_layers\n",
    "        x=layers.Bidirectional(layers.LSTM(\n",
    "                units=RNNunits,\n",
    "                kernel_regularizer = regularizers.l2(regu),\n",
    "                recurrent_regularizer = regularizers.l2(regu),\n",
    "                recurrent_dropout=RNN_dropout, \n",
    "                return_sequences=True)) (stackin)\n",
    "        #a residual connection and normalization step\n",
    "        x=tf.add(stackin, x)\n",
    "        stackin = layers.BatchNormalization() (x) \n",
    "\n",
    "    \n",
    "    #the last layer must have return  sequences = False\n",
    "    x=layers.Bidirectional(layers.LSTM(\n",
    "                units=RNNunits,\n",
    "                kernel_regularizer = regularizers.l2(regu),\n",
    "                recurrent_regularizer = regularizers.l2(regu),\n",
    "                recurrent_dropout=RNN_dropout, \n",
    "                return_sequences=False)) (stackin)\n",
    "    #cannot use residual connection here because of return_sequences=False -- dimensionality not preserved\n",
    "    stackin = layers.BatchNormalization() (x)  \n",
    "  \n",
    "    output = layers.Dense(1) (x) \n",
    "    \n",
    "    model=keras.Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learn_rate), \n",
    "                  loss=\"mse\", \n",
    "                  metrics=[\"mae\", \"mape\"])\n",
    "    return model\n",
    "\n",
    "      \n",
    "        \n",
    "model = RNN_cod_pos_tuner_regr(kt.HyperParameters())\n",
    "\n",
    "\n",
    "#stop if validation loss does not improve for the last patience epochs\n",
    "callbacks_list=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                              patience=patience_glob)]\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "hypermodel=RNN_cod_pos_tuner_regr,\n",
    "objective=\"val_loss\",\n",
    "max_epochs=max_epochs_hptuner_glob,\n",
    "factor=3,\n",
    "hyperband_iterations=3,\n",
    "seed=None,\n",
    "overwrite=True,\n",
    "project_name=\"RNN_cod_pos_tuner_regr\"\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(tr_c, tr_f, \n",
    "             epochs=max_epochs_hptuner_glob, \n",
    "             batch_size=batch_size_glob,\n",
    "             callbacks=callbacks_list,\n",
    "             validation_data=(va_c, va_f),\n",
    "             verbose=0)\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "\n",
    "# load the top  model.\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_tuned_model = models[0]\n",
    "\n",
    "best_tuned_model.summary()\n",
    "\n",
    "\n",
    "#now train the best model for another max_epochs_glob epochs to make sure there was enough training time\n",
    "print(\"\\nnow training best model for more epochs\")\n",
    "history = best_tuned_model.fit(tr_c, tr_f,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(va_c, va_f),\n",
    "                    epochs=max_epochs_glob, \n",
    "                    batch_size=batch_size_glob, \n",
    "                    verbose=0) \n",
    "print(\"trained for additional \", len(history.history['loss']), \" epochs\")\n",
    "\n",
    "print(\"\\nregression quality codon-based RNN with pos embedding\")\n",
    "eval_results = best_tuned_model.evaluate(te_c, te_f, return_dict=True)\n",
    "for metric in eval_results.keys():\n",
    "    print(metric, \" on test set \", eval_results[metric])\n",
    "\n",
    "predict_fit=best_tuned_model.predict(te_c)\n",
    "#calculate correlation coefficients between predicted and actual fitness \n",
    "#necessary to prevent an error when calculating correlation coefficients\n",
    "predict_fit=predict_fit.flatten() \n",
    "print(\"fitness act. vs. pred. for test set\", stats.spearmanr(te_f, predict_fit), \"n=\", len(te_f))      \n",
    "print(\"fitness act. vs. pred. for test set\", stats.pearsonr(te_f, predict_fit), \"n=\", len(te_f))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "## hypertune  a codon-based transformer stack with positional embedding\n",
    "#############################################################################\n",
    "\n",
    "print(\"\\n*******************************************************\")\n",
    "print(\"Now hypertuning codon-based transformer with positional embedding\")\n",
    "print(\"\\n********************************************************\", flush=True)\n",
    "\n",
    "#setting random seeds for each new architecture so that these chunks can be run on their own with\n",
    "#reproducible results if needed\n",
    "random.seed(637281) \n",
    "np.random.seed(123784)\n",
    "tf.random.set_seed(243924)\n",
    "\n",
    "#prepare random training, validation, and test data\n",
    "#Now integer-encode the ntseq, the aa seq and the codon sequence\n",
    "tmpdat=aw.prep_dhfr_data_int_codon_ran(aaseq, fit, sefit, fitshift=2, f_tr=0.5, f_va=0.25, f_te=0.25, hilothresh=1.5)\n",
    "[alldat_ntseq, alldat_aaseq,alldat_codseq,alldat_fit,alldat_sefit,\n",
    "traindat_ntseq,traindat_aaseq,traindat_codseq, traindat_fit,traindat_sefit,\n",
    "valdat_ntseq,valdat_aaseq,valdat_codseq, valdat_fit,valdat_sefit,\n",
    "testdat_ntseq,testdat_aaseq,testdat_codseq, testdat_fit,testdat_sefit,\n",
    "traindat_ntseq_lo,traindat_aaseq_lo,traindat_codseq_lo,traindat_fit_lo,traindat_sefit_lo,\n",
    "valdat_ntseq_lo,valdat_aaseq_lo,valdat_codseq_lo,valdat_fit_lo,valdat_sefit_lo,\n",
    "testdat_ntseq_lo,testdat_aaseq_lo,testdat_codseq_lo,testdat_fit_lo,testdat_sefit_lo,\n",
    "traindat_ntseq_hi,traindat_aaseq_hi,traindat_codseq_hi,traindat_fit_hi,traindat_sefit_hi,\n",
    "valdat_ntseq_hi,valdat_aaseq_hi,valdat_codseq_hi,valdat_fit_hi,valdat_sefit_hi,\n",
    "testdat_ntseq_hi,testdat_aaseq_hi,testdat_codseq_hi,testdat_fit_hi,testdat_sefit_hi]=tmpdat\n",
    "\n",
    "#define convenient acronyms for training data \n",
    "tr_s=traindat_ntseq_hi\n",
    "tr_c=traindat_codseq_hi\n",
    "tr_f=traindat_fit_hi\n",
    "va_s=valdat_ntseq_hi\n",
    "va_c=valdat_codseq_hi\n",
    "va_f=valdat_fit_hi\n",
    "te_s=testdat_ntseq_hi\n",
    "te_c=testdat_codseq_hi\n",
    "te_f=testdat_fit_hi\n",
    "\n",
    "#define the architecture to be hypertuned\n",
    "def transf_cod_pos_tuner_regr(hp):\n",
    "   \n",
    "    emb_dim=hp.Choice(f\"emb_dim\", values=[8, 16, 32, 48])    \n",
    "    n_heads=hp.Choice(f\"n_heads\", values=[2, 4, 6, 8])    \n",
    "    subsp_dim=hp.Choice(f\"subsp_dim\", values=[4, 8, 16, 32, 48])\n",
    "    dense_dim=hp.Choice(f\"dense_dim\", values=[4, 8, 16])\n",
    "    n_stacks=hp.Choice(f\"n_stacks\", values=[1,2, 4, 6])\n",
    "    learn_rate=hp.Choice(f\"learn_rate\", values=lrate_arr_glob)\n",
    "   \n",
    "    if n_stacks<1:\n",
    "        print(\"error_aw: number of stacks invalid\")\n",
    "        exit(1)\n",
    "\n",
    "    inputs = keras.Input(shape=(3,))\n",
    "    embedded=aw.PositionalEmbedding(sequence_length=3, input_dim=64, output_dim = emb_dim) (inputs)\n",
    "    \n",
    "    att_out = layers.MultiHeadAttention(num_heads=n_heads, key_dim=subsp_dim) (embedded, embedded, embedded)\n",
    "    x=tf.add(embedded, att_out)\n",
    "    dense_input = layers.LayerNormalization() (x) \n",
    "    x = layers.Dense(units = dense_dim, activation = 'relu') (dense_input)\n",
    "    dense_output = layers.Dense(units = emb_dim, activation = 'relu') (x)\n",
    "    #a final residual connection and normalization step\n",
    "    x=tf.add(dense_input, dense_output)\n",
    "    stack_out = layers.LayerNormalization() (x)\n",
    "    \n",
    "    #iterate this loop if there is more than one stack\n",
    "    for i in range(n_stacks):\n",
    "        att_out = layers.MultiHeadAttention(num_heads=n_heads, key_dim=subsp_dim) (stack_out, stack_out, stack_out)\n",
    "        x=tf.add(stack_out, att_out)\n",
    "        dense_input = layers.LayerNormalization() (x) \n",
    "        x = layers.Dense(units = dense_dim, activation = 'relu') (dense_input)\n",
    "        dense_output = layers.Dense(units = emb_dim, activation = 'relu') (x)\n",
    "        x=tf.add(dense_input, dense_output)\n",
    "        stack_out = layers.LayerNormalization() (x)\n",
    "        \n",
    "    if hp.Boolean(\"dropout_transformer\"):\n",
    "        stack_out=layers.Dropout(rate=0.1) (stack_out)\n",
    "    \n",
    "    #the last part here is no longer part of the transformer proper\n",
    "    #flatten the layers for the final regression output\n",
    "    x=layers.Flatten() (stack_out)\n",
    "    outputs = layers.Dense(1) (x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)   \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = learn_rate), \n",
    "                  loss=\"mse\", \n",
    "                  metrics=[\"mae\", \"mape\"])\n",
    "    return model\n",
    "    \n",
    "\n",
    "#stop if validation loss does not improve for the last patience epochs\n",
    "callbacks_list=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                              patience=patience_glob)]\n",
    "\n",
    "model = transf_cod_pos_tuner_regr(kt.HyperParameters())\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "hypermodel=transf_cod_pos_tuner_regr,\n",
    "objective=\"val_loss\",\n",
    "max_epochs=max_epochs_hptuner_glob,\n",
    "factor=3,\n",
    "hyperband_iterations=1,\n",
    "seed=None,\n",
    "overwrite=True,\n",
    "project_name=\"transf_cod_pos_tuner_regr\",\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(tr_c, tr_f, \n",
    "             epochs=max_epochs_hptuner_glob, \n",
    "             batch_size=batch_size_glob,\n",
    "             callbacks=callbacks_list,\n",
    "             validation_data=(va_c, va_f),\n",
    "             verbose=0)\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "#load the top  model.\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_tuned_model = models[0]\n",
    "\n",
    "best_tuned_model.summary()\n",
    "\n",
    "#now train the best model for another max_epochs_glob epochs to make sure there was enough training time\n",
    "print(\"\\nnow training best model for more epochs\")\n",
    "history = best_tuned_model.fit(tr_c, tr_f,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(va_c, va_f),\n",
    "                    epochs=max_epochs_glob, \n",
    "                    batch_size=batch_size_glob, \n",
    "                    verbose=0) \n",
    "print(\"trained for additional \", len(history.history['loss']), \" epochs\")\n",
    "\n",
    "\n",
    "print(\"\\nregression quality hypertuned transformer with codon-based pos embedding\")\n",
    "eval_results = best_tuned_model.evaluate(te_c, te_f, return_dict=True)\n",
    "for metric in eval_results.keys():\n",
    "    print(metric, \" on test set \", eval_results[metric])\n",
    "\n",
    "predict_fit=best_tuned_model.predict(te_c)\n",
    "#calculate correlation coefficients between predicted and actual fitness \n",
    "#necessary to prevent an error when calculating correlation coefficients\n",
    "predict_fit=predict_fit.flatten() \n",
    "print(\"fitness act. vs. pred. for test set\", stats.spearmanr(te_f, predict_fit), \"n=\", len(te_f))      \n",
    "print(\"fitness act. vs. pred. for test set\", stats.pearsonr(te_f, predict_fit), \"n=\", len(te_f))      \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
