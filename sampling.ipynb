{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDZdobkbwYAZ"
   },
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "## trains three hypertuned deep learning neural network architectures on training data sets \n",
    "## of different sizes and sampled in multiple different ways. \n",
    "## Determines generalization performance of the resulting neural networks on test data.\n",
    "#############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "#to get reproducible results \n",
    "import random\n",
    "from numpy.random import seed\n",
    "random.seed(637281) #python core random number generator\n",
    "np.random.seed(123784)\n",
    "tf.random.set_seed(243924)\n",
    "\n",
    "from scipy import stats\n",
    "import sys\n",
    "\n",
    "import os #for remove command\n",
    "\n",
    "#for time stamping output file\n",
    "from datetime import datetime\n",
    "dateFORMAT = '%d-%m-%Y'\n",
    "import pandas as pd\n",
    "\n",
    "import deep_funcs_pub as aw\n",
    "#in case aw has changed\n",
    "import importlib\n",
    "importlib.reload(aw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tWnVKFkRbvj",
    "outputId": "e1918efd-c0a7-49fc-f043-bb88bf01faa8"
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "### extract fitness data\n",
    "#########################################################\n",
    "\n",
    "fitdatfile=\"fitness_data_science_papkou2023.tsv\"\n",
    "pathstr=\"\"\n",
    "filepath = pathstr + fitdatfile\n",
    "infile = Path(filepath)\n",
    "df =  pd.read_csv(infile, sep='\\t')\n",
    "\n",
    "\n",
    "fitdatall=df.T.to_dict('list')\n",
    "\n",
    "fit={}\n",
    "sefit={}\n",
    "aaseq={}\n",
    "for n in fitdatall.keys():\n",
    "    ntseq=fitdatall[n][0]\n",
    "    aaseq[ntseq]=fitdatall[n][1]\n",
    "    fit[ntseq]=fitdatall[n][2]\n",
    "    sefit[ntseq]=fitdatall[n][3]\n",
    "\n",
    "#shift fitness by 2 to avoid divergence of mape near zero and for consistency with \n",
    "#the data transformation used during hypertuning\n",
    "fitshift = 2\n",
    "for s in fit.keys():\n",
    "    fit[s]+=2\n",
    "print(\"shifting fitness values by \", fitshift, \n",
    "      \"min/max after shift\", np.min(list(fit.values())), np.max(list(fit.values())) )\n",
    "\n",
    "#now write data for genotypes with fitness above the threshold below into a new dict\n",
    "#these correspond to the viable genotype\n",
    "hilothresh=1.5\n",
    "fit_hi={}\n",
    "sefit_hi={}\n",
    "aaseq_hi={}\n",
    "for s in fit.keys():\n",
    "     if fit[s]>=hilothresh:\n",
    "         aaseq_hi[s]=aaseq[s]\n",
    "         fit_hi[s]=fit[s]\n",
    "         sefit_hi[s]=sefit[s]\n",
    "print(\"number of data points after filtering for fitness above \", hilothresh, \n",
    "      \":\", len(list(fit_hi.values())))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "## main routine: loop over different sampling modes and test set sizes \n",
    "#######################################################################\n",
    "\n",
    "\n",
    "#defines fold-cross validation used during training\n",
    "crossfold=4  \n",
    "print(\"performing \", crossfold, \"-fold cross validation \")\n",
    "\n",
    "#it is useful to have the currently best network stored away\n",
    "#needed to also delete this model for each new training\n",
    "tmp_best_model_file = \"sampling_tmp_best.keras\"\n",
    "\n",
    "#stop nn training if validation mae does not improve for the last patience epochs\n",
    "callbacks_list=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                  patience=5),\n",
    "                keras.callbacks.ModelCheckpoint(filepath=tmp_best_model_file, \n",
    "                                                   monitor=\"val_loss\", \n",
    "                                                   save_best_only=True)]\n",
    "\n",
    "max_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "trva_size_arr=range(200, 8001, 200)\n",
    "\n",
    "\n",
    "#the different sampling modes to be used\n",
    "sampling_mode_arr=[\"random\", \"unique_aas\", \"two_syn_aas\", \"max_codon_usage\", \n",
    "                   \"maxdiv_nts\", \"maxdiv_aas_georgiev\", \"NNK\", \"NNS\", \"NNG\", \n",
    "                   \"NNT\", \"NDT\", \"Tang\"]\n",
    "\n",
    "\n",
    "#an array to hold different NN architectures to try\n",
    "architectures=[\"dense_stack\", \"RNN_stack\", \"transf\"]\n",
    "\n",
    "#number of replicate trainings to be performed for each nn\n",
    "n_replicates=3\n",
    "print(\"performing \", n_replicates, \" replicates\")  \n",
    "\n",
    "#a data frame to which the output will be written\n",
    "datoutarr=[]\n",
    "datoutdf = pd.DataFrame(datoutarr, columns=['architecture', 'sampling_mode', 'size', 'actual_repl', \n",
    "                                            'mean_act_n_sample', 'serr_act_n_sample',\n",
    "                                            'mean_frac_sample','serr_frac_sample',\n",
    "                                            'mean_minave_va_loss', 'serr_minave_va_loss', \n",
    "                                            'mean_te_loss', 'serr_te_loss',\n",
    "                                            'mean_te_mae', 'serr_te_mae', \n",
    "                                            'mean_te_mape', 'serr_te_mape', \n",
    "                                            'mean_te_spe', 'serr_te_spe',\n",
    "                                            'mean_te_pea',  'serr_te_pea'])\n",
    "\n",
    "\n",
    "#loop over nn architectures  \n",
    "for archit in architectures:\n",
    "    #loop over sampling strategies\n",
    "    for s_mode in sampling_mode_arr:\n",
    "\n",
    "        #loop over sample sizes\n",
    "        for size in trva_size_arr:\n",
    "                        \n",
    "            #to free memory from previous sessions\\n\",\n",
    "            tf.keras.backend.clear_session()\n",
    "            try:\n",
    "                del model\n",
    "            except NameError:\n",
    "                pass\n",
    "\n",
    "            \n",
    "            #define the target size for the training|validation (tr/va) data set\n",
    "            f_tr_va=size/len(list(fit_hi.values()))\n",
    "              \n",
    "            #arrays that will hold for each replicate \n",
    "            trva_size=[] #actual size of tr/va set size as fraction of whole data set \n",
    "            trva_frac=[] #fraction of actual tr/va set size as fraction of whole data set\n",
    "            minave_va_loss=[] #min of the average validation loss across all k-fold cross validation \n",
    "            te_loss=[] # test loss\n",
    "            te_mae=[] #test mae\n",
    "            te_mape=[] #test mape\n",
    "\n",
    "            te_spe=[] #spearman correlation between predicted and observed fitness\n",
    "            te_pea=[] #pearson correlation between predicted and observed fitness\n",
    "\n",
    "            \n",
    "            #loop over replicate trainings\n",
    "            for repl in range(n_replicates):\n",
    "                #console output for monitoring\n",
    "                print(\"\\n\\nMODEL:\", archit, \" sampling_mode: \", s_mode, \" size\", size, \n",
    "                      \" replicate\", repl, \"\\n\\n\")\n",
    "\n",
    "                #to record training losses\n",
    "                tr_loss_hist = []\n",
    "                tr_mae_hist = []\n",
    "                tr_mape_hist = []\n",
    "                va_loss_hist = []\n",
    "                va_mae_hist = []\n",
    "                va_mape_hist = []\n",
    "                       \n",
    "\n",
    "                # notice that each replicate uses its own data sample\n",
    "                # the following models can all use data from the same function\n",
    "                if (archit == 'RNN_stack') or (archit == 'dense_stack') or (archit == 'transf'):\n",
    "                    \n",
    "                    [tr, va, te]=aw.dhfr_sample_data_kfold_int_codon_onehot(s_mode, crossfold, aaseq_hi, fit_hi, sefit_hi, \n",
    "                                                                 f_tr_va, f_te=0.5, flattenflag=1) \n",
    "                else:\n",
    "                    print(\"'error_aw': invalid model architecture\")\n",
    "\n",
    "                #delete the checkpoint model from the last training session\n",
    "                #this is so awkward because apptly keras has no command to do that\n",
    "                if os.path.exists(tmp_best_model_file):\n",
    "                    os.remove(tmp_best_model_file)\n",
    "                \n",
    "                #it is important to have this statement here, or the previously deleted best\n",
    "                #solution file will apptly. not be recreated\n",
    "                #stop if validation loss does not improve for the last patience epochs\n",
    "                callbacks_list=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                  patience=5),\n",
    "                                keras.callbacks.ModelCheckpoint(filepath=tmp_best_model_file, \n",
    "                                                   monitor=\"val_loss\", \n",
    "                                                   save_best_only=True)]\n",
    "\n",
    "                    \n",
    "                #when callback is used, the epoch at which the training stopped for each of the k-fold cross-validations\n",
    "                stopped_epoch=[]\n",
    "                for i in range(crossfold):\n",
    "                    \n",
    "                    if archit=='transf':\n",
    "                        model = aw.transf_cod_w_pos_embed_v1()\n",
    "\n",
    "                        tr_c=tr[\"tr_codseq_int\"][i]\n",
    "                        tr_f=tr[\"tr_fit\"][i]\n",
    "                        va_c=va[\"va_codseq_int\"][i]\n",
    "                        va_f=va[\"va_fit\"][i]\n",
    "                        \n",
    "                        history = model.fit(tr_c, tr_f,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    validation_data=(va_c, va_f),\n",
    "                                    epochs=max_epochs, batch_size=batch_size, verbose=0) \n",
    "                        \n",
    "                        \n",
    "                    elif archit=='RNN_stack':\n",
    "                        model = aw.RNN_stack_w_pos_embed_v1()\n",
    "\n",
    "                        tr_c=tr[\"tr_codseq_int\"][i]\n",
    "                        tr_f=tr[\"tr_fit\"][i]\n",
    "                        va_c=va[\"va_codseq_int\"][i]\n",
    "                        va_f=va[\"va_fit\"][i]\n",
    "                        \n",
    "                        history = model.fit(tr_c, tr_f,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    validation_data=(va_c, va_f),\n",
    "                                    epochs=max_epochs, batch_size=batch_size, verbose=0) \n",
    "                        \n",
    "                    elif archit=='dense_stack':\n",
    "                        model = aw.dense_stack_v1()\n",
    "                        #print(\"loaded dense stack model\")\n",
    "                        #print(model.summary())\n",
    "                        \n",
    "                        \n",
    "                        tr_s=tr[\"tr_ntseq_1hot\"][i]\n",
    "                        tr_f=tr[\"tr_fit\"][i]\n",
    "                        va_s=va[\"va_ntseq_1hot\"][i]\n",
    "                        va_f=va[\"va_fit\"][i]\n",
    "                        \n",
    "                        history = model.fit(tr_s, tr_f,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    validation_data=(va_s, va_f),\n",
    "                                    epochs=max_epochs, batch_size=batch_size, verbose=0) \n",
    "                    else:\n",
    "                        print(\"'error_aw': invalid model architecture\")\n",
    "\n",
    "\n",
    "                   \n",
    "                    \n",
    "                    print(\"epoch at which stopping occurred\", callbacks_list[0].stopped_epoch)\n",
    "                    stopped_epoch.append(callbacks_list[0].stopped_epoch)\n",
    "\n",
    "                    tr_loss_hist.append(history.history[\"loss\"])\n",
    "                    va_loss_hist.append(history.history[\"val_loss\"])\n",
    "                    tr_mae_hist.append(history.history[\"mae\"])\n",
    "                    va_mae_hist.append(history.history[\"val_mae\"])\n",
    "                    tr_mape_hist.append(history.history[\"mape\"])\n",
    "                    va_mape_hist.append(history.history[\"val_mape\"])\n",
    "\n",
    "\n",
    "\n",
    "                #when we use a callback for early stopping, otherwise comment out\n",
    "                num_epochs = np.min(stopped_epoch)\n",
    "                \n",
    "                #very occasionally an RNN fails to train, a likely keras bug\n",
    "                #if that happens, print a warning and throw out the entire replicate\n",
    "                if num_epochs==0:\n",
    "                    print(\"\\nwarning_aw: at least one fold cross-training failed, skipping entire replicate\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                #store the history for each of the k-fold trainings\n",
    "                #this will be an array containing the average loss over all k folds over time (up to num_epochs) \n",
    "                ave_tr_loss_hist = [\n",
    "                np.mean([x[i] for x in tr_loss_hist]) for i in range(num_epochs)]\n",
    "                ave_tr_mape_hist = [\n",
    "                np.mean([x[i] for x in tr_mape_hist]) for i in range(num_epochs)]\n",
    "                ave_tr_mae_hist = [\n",
    "                np.mean([x[i] for x in tr_mae_hist]) for i in range(num_epochs)]\n",
    "                ave_va_loss_hist = [\n",
    "                np.mean([x[i] for x in va_loss_hist]) for i in range(num_epochs)]\n",
    "                ave_va_mae_hist = [\n",
    "                np.mean([x[i] for x in va_mae_hist]) for i in range(num_epochs)]\n",
    "                ave_va_mape_hist = [\n",
    "                np.mean([x[i] for x in va_mape_hist]) for i in range(num_epochs)]\n",
    "\n",
    "            \n",
    "                if archit=='hybrid':\n",
    "                    \n",
    "                    best_model=keras.models.load_model(tmp_best_model_file, \n",
    "                                                        custom_objects ={\"PositionalEmbedding\": aw.PositionalEmbedding})\n",
    "                    #define test sets\n",
    "                    te_s=te[\"te_ntseq_1hot\"]\n",
    "                    te_c=te[\"te_codseq_int\"]\n",
    "                    te_f=te[\"te_fit\"] \n",
    "                    te_sefit=te[\"te_sefit\"] \n",
    "\n",
    "                  \n",
    "                    predict_fit=best_model.predict([te_s, te_c])\n",
    "                    eval_results = best_model.evaluate([te_s, te_c], te_f, \n",
    "                                                              return_dict=True,\n",
    "                                                              verbose =0) #2: single line output, 0: silent\n",
    "                elif archit=='dense_stack':\n",
    "                    best_model=keras.models.load_model(tmp_best_model_file) \n",
    "                                                      \n",
    "                                                      \n",
    "                    #define test sets\n",
    "                    te_s=te[\"te_ntseq_1hot\"]\n",
    "                    te_f=te[\"te_fit\"] \n",
    "                    te_sefit=te[\"te_sefit\"] \n",
    "\n",
    "                  \n",
    "                    predict_fit=best_model.predict(te_s)\n",
    "                    eval_results = best_model.evaluate(te_s, te_f, \n",
    "                                                        return_dict=True,\n",
    "                                                        verbose =2) #2: single line output, 0: silent  \n",
    "                \n",
    "                elif archit=='RNN_stack':\n",
    "                    best_model=keras.models.load_model(tmp_best_model_file, \n",
    "                                                        custom_objects ={\"PositionalEmbedding\": aw.PositionalEmbedding})\n",
    "                    #define test sets\n",
    "                    te_c=te[\"te_codseq_int\"]\n",
    "                    te_f=te[\"te_fit\"] \n",
    "                    te_sefit=te[\"te_sefit\"] \n",
    "\n",
    "                  \n",
    "                    predict_fit=best_model.predict(te_c)\n",
    "                    eval_results = best_model.evaluate(te_c, te_f, \n",
    "                                                              return_dict=True,\n",
    "                                                              verbose =2) #2: single line output, 0: silent\n",
    "                    \n",
    "                elif archit=='transf':\n",
    "                    best_model=keras.models.load_model(tmp_best_model_file, \n",
    "                                                        custom_objects ={\"PositionalEmbedding\": aw.PositionalEmbedding})\n",
    "                    #define test sets\n",
    "                    te_c=te[\"te_codseq_int\"]\n",
    "                    te_f=te[\"te_fit\"] \n",
    "                    te_sefit=te[\"te_sefit\"] \n",
    "\n",
    "                  \n",
    "                    predict_fit=best_model.predict(te_c)\n",
    "                    eval_results = best_model.evaluate(te_c, te_f, \n",
    "                                                              return_dict=True,\n",
    "                                                              verbose =2) #2: single line output, 0: silent\n",
    "                else:\n",
    "                    print(\"error_aw: invalid architecture\")\n",
    "                    sys.exit()\n",
    "               \n",
    "                    \n",
    "                #now fill the result arrays for this replicate\n",
    "                \n",
    "                #compute the actual size of the combined training and validation set\n",
    "                trva_size.append(len(tr[\"tr_fit\"][0])+len(va[\"va_fit\"][0]))\n",
    "                trva_frac.append(f_tr_va)\n",
    "                #this computes the minimum over the average loss (over all k folds) for all epochs, which\n",
    "                #is not necessarily the minimum at the last epochs, but probably close.\n",
    "                minave_va_loss.append(np.min(ave_va_loss_hist))\n",
    "\n",
    "                te_loss.append(eval_results[\"loss\"])\n",
    "                te_mae.append(eval_results[\"mae\"])\n",
    "                te_mape.append(eval_results[\"mape\"])\n",
    "\n",
    "                #some versions of the spearmanr and pearsonr routines require flat arrays as inputs\n",
    "                speres=stats.spearmanr(te_f, predict_fit.flatten())\n",
    "                te_spe.append(speres.correlation)\n",
    "\n",
    "                peares=stats.pearsonr(te_f, predict_fit.flatten())\n",
    "                te_pea.append(peares.statistic) #the statistic is itself an array, \n",
    "\n",
    "                for metric in eval_results.keys():\n",
    "                    print(metric, \" on test set\", eval_results[metric])\n",
    "                print(\"fitness act. vs. pred. for test set\", stats.spearmanr(te_f, predict_fit.flatten()), \"n=\", len(te_f))      \n",
    "                print(\"fitness act. vs. pred. for test set\", stats.pearsonr(te_f, predict_fit.flatten()), \"n=\", len(te_f))      \n",
    "\n",
    "            #end of loop over replicates, now calculate statistics over the replicates\n",
    "            \n",
    "            #to deal with situations (rare and only for RNNs) where some replicates were skipped because \n",
    "            #training on at least one fold failed and terminated at epoch zero\n",
    "            actual_repl=len(trva_size)\n",
    "            \n",
    "            if actual_repl>0:\n",
    "                datrow=[archit, s_mode, size, actual_repl,  \n",
    "                        np.mean(trva_size), np.std(trva_size)/np.sqrt(actual_repl), \n",
    "                        np.mean(trva_frac), np.std(trva_frac)/np.sqrt(actual_repl), \n",
    "                        np.mean(minave_va_loss), np.std(minave_va_loss)/np.sqrt(actual_repl), #note that this is an average of averages\n",
    "                        np.mean(te_loss), np.std(te_loss)/np.sqrt(actual_repl), \n",
    "                        np.mean(te_mae), np.std(te_mae)/np.sqrt(actual_repl), \n",
    "                        np.mean(te_mape), np.std(te_mape)/np.sqrt(actual_repl), \n",
    "                        np.mean(te_spe), np.std(te_spe)/np.sqrt(actual_repl), \n",
    "                        np.mean(te_pea), np.std(te_pea)/np.sqrt(actual_repl)]\n",
    "            #for the unlikely case that all replicates have at least one fold crossvalidation that failed to train    \n",
    "            else:\n",
    "                datrow=[archit, s_mode, size, actual_repl,  \n",
    "                        np.nan , np.nan, \n",
    "                        np.nan , np.nan, \n",
    "                        np.nan , np.nan, \n",
    "                        np.nan , np.nan, \n",
    "                        np.nan , np.nan, \n",
    "                        np.nan , np.nan, \n",
    "                        np.nan , np.nan, \n",
    "                        np.nan , np.nan, \n",
    "                        np.nan , np.nan ]\n",
    "            print(datrow)\n",
    "            datoutarr.append(datrow)\n",
    "            #appending the row to the data file\n",
    "            datoutdf.loc[len(datoutdf)] = datrow\n",
    "\n",
    "        #end of loop over all sizes, write copy of the file\n",
    "        datoutfile=\"sampling_\"+ \\\n",
    "                     datetime.now().strftime(dateFORMAT) + \".txt\"\n",
    "        datoutdf.to_csv(datoutfile, sep='\\t', index=False) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
